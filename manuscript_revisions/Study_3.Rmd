---
title             : "Study 3"
shorttitle        : "Moral Dilution"
author:
  - name          : "Blinded"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Blinded"
    email         : "Blinded"
  - name          : "Blinded"
    affiliation   : "2"
  - name          : "Blinded"
    affiliation   : "1"
  - name          : "Blinded"
    affiliation   : "1"
affiliation:
  - id            : "1"
    institution   : "Blinded"
  - id            : "2"
    institution   : "Blinded"
author_note: >
  All procedures performed in studies involving human participants were approved by institutional research ethics committee and conducted in accordance with the Code of Professional Ethics of the Psychological Society of Ireland, and with the 1964 Helsinki declaration and its later amendments or comparable ethical standards. Informed consent was obtained from all individual participants included in the study. The authors declare that there are no potential conflicts of interest with respect to the research, authorship, and/or publication of this article. All authors consented to the submission of this manuscript.
abstract: >
  Six studies etc.
keywords          : "keywords"
wordcount         : "TBC"
bibliography: "../resources/bib/My Library.bib"
csl: "../resources/bib/apa.csl"
figsintext        : true
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
toc               : false
lang              : "en-US"
documentclass     : "apa7"
replace_ampersands: no
output:
  papaja::apa6_pdf
header-includes:
- \raggedbottom
editor_options: 
  chunk_output_type: console
---


```{r S3setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE)
# knitr::opts_chunk$set(eval = TRUE, echo = TRUE)
#knitr::opts_chunk$set(include = FALSE)
```


```{r S3load_libraries_cogload}
rm(list = ls())
library(citr)
#install.packages("sjstats")
library(plyr)
library(foreign)
library(car)
library(desnum)
library(ggplot2)
library(extrafont)
#devtools::install_github("crsh/papaja")
library(papaja)
#library("dplyr")
library("afex")
library("tibble")
library(scales)
#install.packages("metap")
library(metap)
library(pwr)
library(lsr)
#install.packages("sjstats")
library(sjstats)
library(DescTools)
#inatall.packages("ggstatsplot")
#library(ggstatsplot)
library(VGAM)
library(nnet)
library(mlogit)
library(reshape2)
#install.packages("powerMediation")
library("powerMediation")
library("ggpubr")
library(tidyverse)
library("ftExtra")



#remotes::install_github("atusy/ftExtra")

# library(rstatix)


#source("load_all_data.R")

#devtools::install_github("benmarwick/wordcountaddin")
#library(wordcountaddin)
#wordcountaddin::text_stats("cogload_1to5_25Sept19.Rmd")
#setwd("manuscript_prep")
getwd()
```



```{r S3LoadData}
#source("~/Dropbox/College/research/Research_general/cog_load/moral_dumbfounding_and_cognitive_load/read_and_sort_raw_data.R")
#source("~/Dropbox/College/research/Research_general/cog_load/moral_dumbfounding_and_cognitive_load/load_study6_data.R")


# write.csv(df_wide,       "data/study3_rep_data_wide.csv", row.names = FALSE)
# write.csv(df_long,       "data/study3_rep_data_long.csv", row.names = FALSE)
# write.csv(df_long_clean, "data/study3_rep_data_long_clean.csv", row.names = FALSE)
# write.csv(df_wide_clean, "data/study3_rep_data_wide_clean.csv", row.names = FALSE)
# write.csv(df_long_failed, "data/study3_rep_data_long_failed.csv", row.names = FALSE)


rm(list = ls())

#df_long <- read.csv("../data/study6_data_long.csv")
#df_wide <- read.csv("../data/study6_data_wide.csv")
#df_long_clean <- read.csv("../data/study6_data_long_clean.csv")

#df3 <- df_long # read.csv("../data/study3_rep_data_long.csv")
#df1 <- read.csv("../data/study6_data_wide.csv")
#x <- read.csv("../data/study6_data_long_clean.csv")

df_long <- read.csv("../data/study3_data_long.csv")
df_wide <- read.csv("../data/study3_data_wide.csv")
df_long_clean <- read.csv("../data/study3_data_long_clean.csv")

df3 <- df_long # read.csv("../data/Study3_data_long.csv")
df1 <- read.csv("../data/study3_data_wide.csv")
x <- read.csv("../data/study3_data_long_clean.csv")


ID_count <- dplyr::count(x,x$ResponseId)
ID_count <- `colnames<-`(ID_count,c("ResponseId","count"))

x <- left_join(x,ID_count, by = "ResponseId")
x <- x[which(x$count==4),]
rm(ID_count)

df_complete <- x

df3 <- df_long

MPS <- x %>% 
  select(R1,R2,R3,R4)

alpha1 <- ltm::cronbach.alpha(MPS)

alpha1
```

# Study 3 - Good and Bad Characters
In Study 1 we found evidence for the moral dilution effect for judgments of *bad* moral characters. In Study 2 we failed replicate this effect for judgments of *good* moral characters. The aim of Study 3 was to test if valence (good vs. bad) moderates the moral dilution effect. We hypothesized that valence (good vs bad) would interact with condition in producing a dilution effect, such that the dilution effect would be observed for bad characters but not for good characters. Study 3 was pre-registered at \color{blue}[https://aspredicted.org/QDF_XT1](https://aspredicted.org/QDF_XT1)\color{black}.


## Study 3: Method
### Study 3: Participants and design
Study 3 was a 2 $\times$ 2 within-subjects factorial design. The first independent variable was condition with two levels, diagnostic and non-diagnostic. The second independent variable was valence of character description, with two levels morally good and morally bad. We used the same two dependent variables as in previous studies, the four item moral perception scale (MPS-4, $\alpha$ = `r round(alpha1$alpha,digits=2)`), and the single item moral perception measure MM-1.

A total sample of `r length(levels(as.factor(df3$ResponseId)))` (`r sum(df1$gender=="2",na.rm=T)` female, `r sum(df1$gender=="1", na.rm=T)` male, `r round(sum(df1$gender=="3", na.rm=T)/1)` non-binary, `r round(sum(df1$gender=="4", na.rm=T))` other; `r sum(df1$gender=="5", na.rm=T)` prefer not to say, *M*~age~ = `r round(mean(df3$age, na.rm=T),digits=2)`, min = `r min(df3$age, na.rm=T)`, max = `r max(df3$age, na.rm=T)`, *SD* = `r round(sd(df3$age, na.rm=T),digits=2)`) started the survey.  Participants were recruited from Prolific Academic and paid $0.40 for their participation.

```{r}
# df1 <- read.csv("../data/study6_data_long.csv")
# df3 <- read.csv("../data/study6_data_long_clean.csv")

df1 <- read.csv("../data/study3_data_long.csv")
df3 <- read.csv("../data/study3_data_long_clean.csv")

df_long_clean <- df3
table(df3$gender)
length(df1$gender)/6
length(levels(as.factor(df1$ResponseId)))-length(levels(as.factor(df3$ResponseId)))
length(df3$gender)/4

att_both <- length(levels(as.factor(df1$ResponseId)))-length(levels(as.factor(df3$ResponseId)))
```


```{r}
df_long_clean <- df3

x <- df3
x$attn_chk_1Q
x$attn_chk_2_Q
x <- x[which(x$attn_chk_2_Q==2|x$attn_chk_2_Q==5),]
x <- x[which(x$attn_chk_1Q==7),]
df_long_extra_clean <- x

x <- df3
# 
# df3 <- x
# att_both <- length(levels(as.factor(df1$ResponseId)))-length(levels(as.factor(df3$ResponseId)))


good <- x[which(x$valence=="good"),]
bad <- x[which(x$valence=="bad"),]

good$R_tot_recoded <- 7 - good$R_tot
bad$R_tot_recoded <- bad$R_tot

good$M1_recoded <- 100 - good$M1
bad$M1_recoded <- bad$M1

df_recoded <- rbind(good,bad)


x <- df_recoded # df_long_extra_clean

good <- x[which(x$valence=="good"),]
bad <- x[which(x$valence=="bad"),]

good$R_tot_recoded <- 7 - good$R_tot
bad$R_tot_recoded <- bad$R_tot

good$M1_recoded <- 100 - good$M1
bad$M1_recoded <- bad$M1

df_recoded_extra_clean <- rbind(good,bad)

x <- df_recoded_extra_clean

df3 <- df_complete
```

Participants who failed both manipulation checks (*n* = `r att_both`), or did not complete all measures were removed , leaving a total sample of `r length(df3$gender)/4` participants (`r sum(df3$gender=="2",na.rm=T)/4` female, `r sum(df3$gender=="1", na.rm=T)/4` male, `r sum(df3$gender=="4", na.rm=T)/4` other, `r sum(df3$gender=="4", na.rm=T)/4` prefer not to say; *M*~age~ = `r round(mean(df3$age, na.rm=T),digits=2)`, min = `r min(df3$age, na.rm=T)`, max = `r max(df3$age, na.rm=T)`, *SD* = `r round(sd(df3$age, na.rm=T),digits=2)`).


### Study 3: Procedure and materials
Again, data were collected using an online questionnaire presented with Qualtrics (www.qualtrics.com). Participants were presented with four descriptions of characters taken from Studies 1 and 2. To ensure consistency across character judgments, we selected descriptions that related to the same moral foundations (care, fairness, and loyalty). We used the same four character names as in previous studies. The *good* characters were *Sam* and *Robin*, and the *bad* characters were *Francis* and *Alex*, e.g., *Imagine a person named Robin. Throughout their life they have been known to show compassion and empathy for others, act with a sense of fairness and justice, and, never to break their word.* or, *Imagine a person named Alex. Throughout their life they have been known to be cruel, act unfairly, and to betray their own group.* Full descriptions for each character are in the supplementary materials. One description for each the *good* and *bad* characters was randomly assigned to include non-diagnostic information for each participant thus all participants were exposed to all conditions (see \color{blue}[https://osf.io/mdnpv/?view_only=77883e3fbc3d45f1a35fe92d5318cb67](https://osf.io/mdnpv/?view_only=77883e3fbc3d45f1a35fe92d5318cb67)\color{black}  for details of the randomization blocks). Study 3 was pre-registered at \color{blue}[https://aspredicted.org/QDF_XT1](https://aspredicted.org/QDF_XT1)\color{black}


## Study 3: Results



```{r}

x <- bad

sum(x$R_tot>4)
length(x$R_tot)
length(unique(x$ResponseId))*2

sum(x$R_tot>4)/(length(unique(x$ResponseId))*2)
sum(x$M1>50)/(length(unique(x$ResponseId))*2)



totalpp <- length(df3$gender)/4 #length(unique(x$ResponseId))
totalju <- (length(df3$gender)/2)

mps4tot <- sum(x$R_tot>4)
mps4prop <- (sum(x$R_tot>4)/(length(df3$gender)/2))*100

mm1tot <- sum(x$M1>50)
mm1prop <- (sum(x$M1>50)/(length(df3$gender)/2))*100

```



As in Studies 1 and 2 we assessed the quality of the data by examining the extent to which responses fell above / below the midpoint for the *bad* / *good* descriptions respectively. All *N* = `r totalpp` participants responded to two *bad* descriptions and 2 *good* descriptions, resulting in a total of `r totalju` responses for each measure for both *bad* and *good* descriptions.
Taking the *bad* descriptions first, for MPS-4, `r mps4tot` (`r mps4prop`%) responses were above the midpoint, and for MM-1, `r mm1tot` (`r mm1prop`%) were above the midpoint. 







```{r}

x <- good

sum(x$R_tot<4)/(length(unique(x$ResponseId))*2)
sum(x$M1<50)/(length(unique(x$ResponseId))*2)


totalpp <- length(unique(x$ResponseId))
totalju <- (length(df3$gender)/2)

mps4tot <- sum(x$R_tot<4)
mps4prop <- (sum(x$R_tot<4)/(length(df3$gender)/2))*100

mm1tot <- sum(x$M1<50)
mm1prop <- (sum(x$M1<50)/(length(df3$gender)/2))*100

```

Regarding the *good* descriptions, for MPS-4, `r mps4tot` (`r mps4prop`%) responses were below the midpoint, and for MM-1, `r mm1tot` (`r mm1prop`%) were below the midpoint.


```{r}
x <- df3
sam <- x[which(x$scenario=="sam"),]
francis <- x[which(x$scenario=="francis"),]
alex <- x[which(x$scenario=="alex"),]
robin <- x[which(x$scenario=="robin"),]

```


In order to test for the information type × valence interaction effect in a single model we recoded both MPS-4 and MM-1 into two new variables MPS-4R, and MM-1R. These recoded variables were the same as the original variables but the responses to the good characters were reverse coded. This allowed us to examine whether the dilution effect was different depending on whether participants were judging good characters or bad characters without this analysis being confounded by valence.




```{r}
# https://rpkgs.datanovia.com/rstatix/reference/factorial_design.html

x <- df_recoded

table(x$scenario,x$ResponseId)
table(x$ResponseId)

duplicated(x$ResponseId)
levels(as.factor(x$ResponseId))[2]


pid_len_fun <- function(y){
    sum(x$ResponseId==levels(as.factor(x$ResponseId))[y])<4}



sum(x$ResponseId==levels(as.factor(x$ResponseId))[1])<4

pid_len_fun(1)

lapply(c(1:10), pid_len_fun)


test <- cbind.data.frame(levels(as.factor(x$ResponseId)), 
                 unlist(
                   lapply(c(1:length(levels(as.factor(x$ResponseId))))
                          , pid_len_fun)))

#x = 
to_keep <- test[1][which(test[2]==FALSE),]


#x %>% filter(ResponseId==c(test[1][which(test[2]==FALSE),]))

test <- `colnames<-`(test, c("ResponseId","full"))

full_responses <- left_join(x, test, by = "ResponseId")

full_responses <- full_responses[which(full_responses$full==FALSE),]

x <- full_responses

tapply(x$R_tot_recoded, x$scenario, descriptives)
# bad <- x[which(x$condition=="diagnostic"),]
# good <- x[which(x$condition=="non-diagnostic"),]
#good$scenario_abb <- droplevels(good$scenario_abb)
x$valence
#x <- bad


table(x$condition,x$valence)
x$valence <- as.factor(x$valence)
x$condition <- as.factor(x$condition)
# 
# aov_full <- rstatix::anova_test(
#   data=x
#   , dv=R_tot_recoded
#   , wid = ResponseId
#   , within = condition:valence)
# 
# aov1 <- rstatix::get_anova_table(aov_full)
# aov1
# aov1$DFd

fact_aov <- 
  rstatix::factorial_design(
    x
    , dv = R_tot_recoded
    , wid = ResponseId
    , within = c(condition, valence))


res.anova <- car::Anova(fact_aov$model
                        , idata = fact_aov$idata
                        , idesign = fact_aov$idesign
                        , type = 3)

summary(res.anova, multivariate = FALSE)
aov1 <- summary(res.anova, multivariate = FALSE)

aov2 <- rstatix::get_anova_table(aov1$univariate.tests)

aov3 <- as.data.frame(as.matrix(aov2)[c(1:4),])
aov3$`Sum Sq`

aov3 <- 
  `colnames<-`(aov3, c(
    "sumsq"
    ,"numdf"
    ,"errorSS"
    ,"dendf"
    ,"F"
    ,"p"
  ))



es <- effectsize::eta_squared(res.anova, alternative = "two.sided" )

es

help(eta_squared)

es$Parameter

es$Eta2_partial
es$CI_low
es$CI_high

es
# 
# t1$t.statistic
# 
# #rstatix::tukey_hsd(res.anova)
# 
# # 
# # 
# x$pair <- as.factor(paste(x$condition,x$valence))
# table(x$pair)
# class(x$pair)
# 
# y <- x[order(as.character(x$ResponseId)),]
# 
# 
# 
# 
# x$pair
# x$scenario
# pwc <- y %>%
#   rstatix::pairwise_t_test(
#     R_tot_recoded ~ pair
#     , paired = TRUE
#     , p.adjust.method = "bonferroni"
#     , pool.sd = FALSE
#     , detailed = TRUE
#     )
# pwc
# p_report(pwc$p[2])
# 
# 
# t1_data <- x[which(x$valence=="good")]
# 
# # d1 <- lsr::cohensD(R_tot~condition,x,method="paired")
# # t.test(R_tot~condition,x,paired=TRUE)
# # t1 <- t.test(x$R_tot~x$scenario,paired=TRUE)
# # d1
# lapply(pwc$p.adj,p_report)


```



First we conducted a within-subjects factorial ANOVA to test for differences in responses to MPS-4R depending on information type and valence. 
There was a main effect for condition,
*F*(`r aov3$numdf[2]`, `r aov3$dendf[2]`)
= `r aov3$F[2]`,
*p* `r paste(p_report(aov3$p[2]))`,
partial $\eta$^2^ = `r es$Eta2_partial[1]`,
95% CI [`r es$CI_low[1]`, `r es$CI_high[1]`],
and a main effect for valence,
*F*(`r aov3$numdf[3]`, `r aov3$dendf[3]`)
= `r aov3$F[3]`,
*p* `r paste(p_report(aov3$p[3]))`,
partial $\eta$^2^ = `r es$Eta2_partial[2]`,
95% CI [`r es$CI_low[2]`, `r es$CI_high[2]`].
There was a significant $\times$ valence interaction effect,
*F*(`r aov3$numdf[4]`, `r aov3$dendf[4]`)
= `r aov3$F[4]`,
*p* `r paste(p_report(aov3$p[4]))`,
partial $\eta$^2^ = `r es$Eta2_partial[3]`,
95% CI [`r es$CI_low[3]`, `r es$CI_high[3]`].




```{r}


x <- full_responses

x <- x[which(x$valence=="bad"),]


x$ResponseId <- as.factor(x$ResponseId)
x$condition <- as.factor(x$condition)

t1 <- lsr::pairedSamplesTTest(R_tot ~ condition
                        ,data = x
                        ,id = "ResponseId")
t1$df
t1$p.value

t1$effect.size

```

Follow-up pairwise t-tests indicated that for the *bad* characters there were significant differences in MPS-4 responses depending on information type
*t*(`r t1$df`),
= `r t1$t.statistic`,
*p* `r paste(p_report(t1$p.value))`
(*p*~adjusted~ `r paste(p_report(t1$p.value*4))`),
*d* = `r t1$effect.size`,
95% CI [`r t1$conf.int[1]`, `r t1$conf.int[2]`].
MPS-4 responses were higher in the non-diagnostic condition
(*M* = `r mean(x$R_tot[which(x$condition=="non-diagnostic")])`,
*SD* = `r sd(x$R_tot[which(x$condition=="non-diagnostic")])`)
compared to the diagnostic condition
(*M* = `r mean(x$R_tot[which(x$condition=="diagnostic")])`,
*SD* = `r sd(x$R_tot[which(x$condition=="diagnostic")])`).

```{r}


x <- full_responses

x <- x[which(x$valence=="good"),]


x$ResponseId <- as.factor(x$ResponseId)
x$condition <- as.factor(x$condition)

t1 <- lsr::pairedSamplesTTest(R_tot ~ condition
                        ,data = x
                        ,id = "ResponseId")
t1$df
t1$p.value
t1$conf.int[1]

```

Similarly, for *good* characters, were significant differences in MPS-4 responses depending on information type
*t*(`r t1$df`),
= `r t1$t.statistic`,
*p* `r paste(p_report(t1$p.value))`
(*p*~adjusted~ `r paste(p_report(t1$p.value*4))`),
*d* `r t1$effect.size`,
95% CI [`r t1$conf.int[1]`, `r t1$conf.int[2]`].
MPS-4 responses were lower in the non-diagnostic condition
(*M* = `r mean(x$R_tot[which(x$condition=="non-diagnostic")])`,
*SD* = `r sd(x$R_tot[which(x$condition=="non-diagnostic")])`)
compared to the diagnostic condition
(*M* = `r mean(x$R_tot[which(x$condition=="diagnostic")])`,
*SD* = `r sd(x$R_tot[which(x$condition=="diagnostic")])`).







```{r}
# https://rpkgs.datanovia.com/rstatix/reference/factorial_design.html

x <- full_responses



table(x$condition,x$valence)
x$valence <- as.factor(x$valence)
x$condition <- as.factor(x$condition)

fact_aov <- 
  rstatix::factorial_design(
    x
    , dv = M1_recoded
    , wid = ResponseId
    , within = c(condition, valence))



res.anova <- car::Anova(fact_aov$model
                        , idata = fact_aov$idata
                        , idesign = fact_aov$idesign
                        , type = 3)

summary(res.anova, multivariate = FALSE)
aov1 <- summary(res.anova, multivariate = FALSE)

aov2 <- rstatix::get_anova_table(aov1$univariate.tests)

aov3 <- as.data.frame(as.matrix(aov2)[c(1:4),])
aov3$`Sum Sq`

aov3 <- 
  `colnames<-`(aov3, c(
    "sumsq"
    ,"numdf"
    ,"errorSS"
    ,"dendf"
    ,"F"
    ,"p"
  ))

res.anova <- car::Anova(fact_aov$model
                        , idata = fact_aov$idata
                        , idesign = fact_aov$idesign
                        , type = 3)
summary(res.anova, multivariate = FALSE)

es <- effectsize::eta_squared(res.anova, alternative = "two.sided" )

es$Parameter



```






Next we conducted a within-subjects factorial ANOVA to test for differences in responses to MM-1R depending on information type and valence. 
There was a main effect for condition,
*F*(`r aov3$numdf[2]`, `r aov3$dendf[2]`)
= `r aov3$F[2]`,
*p* `r paste(p_report(aov3$p[2]))`,
partial $\eta$^2^ = `r es$Eta2_partial[1]`,
95% CI [`r es$CI_low[1]`, `r es$CI_high[1]`],
and a main effect for valence,
*F*(`r aov3$numdf[3]`, `r aov3$dendf[3]`)
= `r aov3$F[3]`,
*p* `r paste(p_report(aov3$p[3]))`,
partial $\eta$^2^ = `r es$Eta2_partial[2]`,
95% CI [`r es$CI_low[2]`, `r es$CI_high[2]`].
There was no condition $\times$ valence interaction effect,
*F*(`r aov3$numdf[4]`, `r aov3$dendf[4]`)
= `r aov3$F[4]`,
*p* `r paste(p_report(aov3$p[4]))`,
partial $\eta$^2^ = `r es$Eta2_partial[3]`,
95% CI [`r es$CI_low[3]`, `r es$CI_high[3]`].




```{r}


x <- full_responses

x <- x[which(x$valence=="bad"),]


x$ResponseId <- as.factor(x$ResponseId)
x$condition <- as.factor(x$condition)

t1 <- lsr::pairedSamplesTTest(M1 ~ condition
                        ,data = x
                        ,id = "ResponseId")
t1$df
t1$p.value

t1$effect.size

```

Follow-up pairwise t-tests indicated that for the *bad* characters there were significant differences in MM-1 responses depending on information type
*t*(`r t1$df`),
= `r t1$t.statistic`,
*p* `r paste(p_report(t1$p.value))`
(*p*~adjusted~ `r paste(p_report(t1$p.value*4))`),
*d* = `r t1$effect.size`,
95% CI [`r t1$conf.int[1]`, `r t1$conf.int[2]`].
MM-1 responses were higher in the non-diagnostic condition
(*M* = `r mean(x$M1[which(x$condition=="non-diagnostic")])`,
*SD* = `r sd(x$M1[which(x$condition=="non-diagnostic")])`)
compared to the diagnostic condition
(*M* = `r mean(x$M1[which(x$condition=="diagnostic")])`,
*SD* = `r sd(x$M1[which(x$condition=="diagnostic")])`).

```{r}


x <- full_responses

x <- x[which(x$valence=="good"),]


x$ResponseId <- as.factor(x$ResponseId)
x$condition <- as.factor(x$condition)

t1 <- lsr::pairedSamplesTTest(M1 ~ condition
                        ,data = x
                        ,id = "ResponseId")
t1$df
t1$p.value
t1$conf.int[1]

```

Similarly, for *good* characters, there were significant differences in MM-1 responses depending on information type
*t*(`r t1$df`),
= `r t1$t.statistic`,
*p* `r paste(p_report(t1$p.value))`
(*p*~adjusted~ `r paste(p_report(t1$p.value*4))`),
*d* = `r t1$effect.size`,
95% CI [`r t1$conf.int[1]`, `r t1$conf.int[2]`].
MM-1 responses were lower in the non-diagnostic condition
(*M* = `r mean(x$M1[which(x$condition=="non-diagnostic")])`,
*SD* = `r sd(x$M1[which(x$condition=="non-diagnostic")])`)
compared to the diagnostic condition
(*M* = `r mean(x$M1[which(x$condition=="diagnostic")])`,
*SD* = `r sd(x$M1[which(x$condition=="diagnostic")])`).





```{r}
x <- df3
# bad <- x[which(x$condition=="diagnostic"),]
# good <- x[which(x$condition=="non-diagnostic"),]
#good$scenario_abb <- droplevels(good$scenario_abb)

x <- full_responses

#x <- bad
aov_full <- rstatix::anova_test(
  data=x
  , dv=M1
  , wid = ResponseId
  , within = scenario)
aov1 <- rstatix::get_anova_table(aov_full)
aov1
aov1$DFd

pwc <- x %>%
  rstatix::pairwise_t_test(
    M1 ~ scenario, paired = TRUE,
    p.adjust.method = "bonferroni"
    )
pwc
p_report(pwc$p[2])
lapply(pwc$p.adj, p_report)
paste(pwc$p.adj[2])
pwc$p[2]
# d1 <- lsr::cohensD(M1~condition,x,method="paired")
# t.test(M1~scenario,x,paired=TRUE)
# t1 <- t.test(x$M1~x$scenaro,paired=TRUE)
# d1

```




```{r}
x <- df3
x <- full_responses

#x <- df_recoded_extra_clean


model0 <- lmerTest::lmer(R_tot_recoded ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
          #      , contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
# model1 <- lmerTest::lmer(R_tot ~
#                   condition*scenario
#                 + (1|ResponseId)
#                 + (1|ResponseId:condition)
#                 , data = x
#                 , contrasts = list(condition = contr.sum  )#, valence = contr.sum)
#             )



model1 <- lmerTest::lmer(R_tot_recoded ~
                  condition*valence
                + (1|ResponseId)
                + (1|ResponseId:condition)
                + (1|ResponseId:valence)
                , data = x
                , contrasts = list(condition = contr.sum, valence = contr.sum )#, valence = contr.sum)
            )
summary(model1)
anova(model1)


model1_std <- lmerTest::lmer(scale(R_tot_recoded, scale = TRUE) ~
                  condition*valence
                + (1|ResponseId)
                + (1|ResponseId:condition)
                + (1|ResponseId:valence)
                #+ (1|scenario)
                , data = x
                , contrasts = list(condition = contr.sum  , valence = contr.sum)
            )
summary(model1_std)
b_std <- as.data.frame(summary(model1_std)$coefficients)$Estimate[2]

results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)
summary_model1
results_coef <- as.data.frame(summary_model1$coefficients)
results_coef

aov1 <- anova(model1)
f3a <- aov1$`F value`[1]
f3b <- aov1$`F value`[2]
f3c <- aov1$`F value`[3]
p3a <- aov1$`Pr(>F)`[1]
p3b <- aov1$`Pr(>F)`[2]
p3c <- aov1$`Pr(>F)`[3]

results_coef$Estimate[2]
results_coef$`Std. Error`[2]
results_coef$df[2]
t1 <- results_coef$`t value`[2]
p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)

s3both_rtot_icc <- performance::icc(model1)

s3both_rtot_vars <- as.data.frame(VarCorr(model1))

```

We conducted a linear-mixed-effects model to test if our predictors influenced MPS-4R responses. Our outcome measure was MPS-4R, our predictor variables were condition and valence; we allowed intercepts and the effects of condition and valence to vary across participants, and we included random effects for scenario.
Overall, the model significantly predicted participants responses, and provided a better fit for the data than the baseline model,
$\chi$^2^(`r results_anova$Df[2]`) = `r results_anova$Chisq[2]`, *p* `r paste(p_report(p1))`. 
Overall, there was a significant main effect for condition,
*F*(`r aov1$NumDF[1]`, `r round(aov1$DenDF[1])`) = `r f3a`, *p* `r paste(p_report(p3a))`;
valence significantly predicted responses,
*F*(`r aov1$NumDF[2]`, `r round(aov1$DenDF[2])`) = `r f3b`, *p* `r paste(p_report(p3b))`;
and there was a significant condition $\times$ valence interaction,
*F*(`r aov1$NumDF[3]`, `r round(aov1$DenDF[3])`) = `r f3c`, *p* `r paste(p_report(p3c))`.



```{r}
x <- df3
x <- full_responses



x$R_tot_extremity <- sqrt((4-x$R_tot)^2)

x$M1_extremity <- sqrt((50-x$M1)^2)


model0 <- lmerTest::lmer(M1_extremity ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
          #      , contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
model1 <- lmerTest::lmer(M1_extremity ~
                  condition*scenario
                + (1|ResponseId)
                + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  )#, valence = contr.sum)
            )



model1 <- lmerTest::lmer(M1_extremity ~
                  condition*valence
                + (1|ResponseId)
                + (1|ResponseId:condition)
                + (1|ResponseId:valence)
                , data = x
                , contrasts = list(condition = contr.sum, valence = contr.sum )#, valence = contr.sum)
            )


model0 <- lmerTest::lmer(M1 ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
          #      , contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model1)

model1 <- lmerTest::lmer(M1 ~
                  condition*scenario
                + (1|ResponseId)
                + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  )#, valence = contr.sum)
            )

x$M1_recoded



model0 <- lmerTest::lmer(M1_recoded ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
          #      , contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

model1 <- lmerTest::lmer(M1_recoded ~
                  condition*valence
                + (1|ResponseId)
                + (1|ResponseId:condition)
                + (1|ResponseId:valence)
                , data = x
                , contrasts = list(condition = contr.sum, valence = contr.sum )#, valence = contr.sum)
            )
summary(model1)
anova(model1)
results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)
summary_model1
results_coef <- as.data.frame(summary_model1$coefficients)
results_coef

aov1 <- anova(model1)
f3a <- aov1$`F value`[1]
f3b <- aov1$`F value`[2]
f3c <- aov1$`F value`[3]
p3a <- aov1$`Pr(>F)`[1]
p3b <- aov1$`Pr(>F)`[2]
p3c <- aov1$`Pr(>F)`[3]

results_coef$Estimate[2]
results_coef$`Std. Error`[2]
results_coef$df[2]
t1 <- results_coef$`t value`[2]
p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)


s3both_M1_vars <- as.data.frame(VarCorr(model1))


s3both_rtot_icc <- performance::icc(model1)

s3both_M1_icc <- performance::icc(model1)


save(s3both_M1_vars, s3both_rtot_vars, s3both_rtot_icc, s3both_M1_icc, file = "s3both_table_data.RData")


```

We conducted a linear-mixed-effects model to test if our predictors influenced MM-1R responses. The model was the same as the previous model, with a change to the outcome measure, our outcome measure for this model was MM-1R. As above, our predictor variables were condition and valence; we allowed intercepts and the effects of condition and valence to vary across participants, and we included random effects for scenario.
Overall, the model significantly predicted participants responses, and provided a better fit for the data than the baseline model,
$\chi$^2^(`r results_anova$Df[2]`) = `r results_anova$Chisq[2]`, *p* `r paste(p_report(p1))`. 
Overall there was a main effect for condition,
*F*(`r aov1$NumDF[1]`, `r round(aov1$DenDF[1])`) = `r f3a`, *p* `r paste(p_report(p3a))`;
valence significantly predicted responses,
*F*(`r aov1$NumDF[2]`, `r round(aov1$DenDF[2])`) = `r f3b`, *p* `r paste(p_report(p3b))`;
and there was no significant condition $\times$ valence interaction,
*F*(`r aov1$NumDF[3]`, `r round(aov1$DenDF[3])`) = `r f3c`, *p* `r paste(p_report(p3c))`.

Interestingly, there was a consistent effect for both condition and valence, as well as a condition $\times$ valence interaction effect. To further examine this interaction effect, we report separate analyses for the good and bad descriptions below.

## Differences in the *Bad* Descriptions





```{r}
x <- df3

#x <- df_long_extra_clean

x <- x[which(x$valence=="bad"),]


model0 <- lmerTest::lmer(R_tot ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
                #, contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
model1 <- lmerTest::lmer(R_tot ~
                  condition*scenario
                + (1|ResponseId)
                # + (1|scenario)
             #   + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  , scenario = contr.sum)
            )
anova(model1)
summary(model1)



model1_std <- lmerTest::lmer(scale(R_tot, scale = TRUE) ~
                  condition*scenario
                + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  , scenario = contr.sum)
            )
summary(model1_std)
b_std <- as.data.frame(summary(model1_std)$coefficients)$Estimate[2]


results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)


results_coef <- as.data.frame(summary_model1$coefficients)

aov1 <- anova(model1)
f3 <- aov1$`F value`[1]
p3 <- aov1$`Pr(>F)`[1]


results_coef$Estimate[2]
results_coef$`Std. Error`[2]
results_coef$df[2]
results_coef$`t value`[2]
t1 <- results_coef$`t value`[2]

p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)


# d <- (mean(x$R_tot[which(x$condition=="diagnostic")])-mean(x$R_tot[which(x$condition=="non-diagnostic")]))/sd(x$R_tot)


s_pooled_fun <- function(){
  
  x$dv <- x$R_tot
  
  
  z <- 1.959964
  
  df_1 <- x[which(x$condition=="diagnostic"),]
  df_2 <- x[which(x$condition=="non-diagnostic"),]
  
  
  df_1 <- df_1[order(df_1$ResponseId), ]
  df_2 <- df_2[order(df_2$ResponseId), ]
  
  x1 <- mean(df_1$dv)
  x2 <- mean(df_2$dv)
  
  n1 <- length(df_1$gender)
  n2 <- length(df_2$gender)
  
  s1 <- sd(df_1$dv)
  s2 <- sd(df_2$dv)
  
  s_pooled <- sqrt(
    (
      ((s1*s1)*(n1-1))+((s2*s2)*(n2-1))
    )/
      (n1+n1-2)
  )
  
  d <- (x1-x2)/s_pooled
  
  
  d_paired <- (
    mean(df_1$dv-df_2$dv)/
      sd(df_1$dv-df_2$dv)
  )
  
  d <- d_paired
  
  vd <- ((n1+n2)/(n1*n2))+((d*d)/(2*(n1+n2)))
  
  sed <- sqrt(vd)
  
  ci_upper <- d+(z*sed)
  ci_lower <- d-(z*sed)
  
  d_paired <- (
    mean(df_1$dv-df_2$dv)/
      sd(df_1$dv-df_2$dv)
  )
  
  
  n <- length(x$gender)/4
  var <- (1/n)+((d_paired*d_paired)/(2*n))
  vd <- var
  sed <- sqrt(vd)
  
  
  ci_upper <- d_paired+(z*sed)
  ci_lower <- d_paired-(z*sed)
  
  list(study="Study 3",
       x1=x1
       ,x2=x2
       ,s1=s1
       ,s2=s2
       ,n1=n1
       ,n2=n2
       ,s_pooled=s_pooled
       ,d=d
       ,vd=vd
       ,sed=sed
       ,ci_upper=ci_upper
       ,ci_lower=ci_lower
       ,p=p2
       ,d_paired=d_paired)
     
}

s3bad_rtot_stats <- s_pooled_fun()
d <- s3bad_rtot_stats$d

```



```{r}

y <- s3bad_rtot_stats

icc <- performance::icc(model1)
icc$ICC_unadjusted

names_for_table <- c("b", "SE", "df", "t", "p"
  , "Mdiag", "SDdiag", "Mnond", "SDnond"
  , "d", "upper","lower","variance","ICC")

results_coef_for_table <- results_coef[2,]
class(results_coef_for_table)

s3bad_rtot_for_table <- 
  `colnames<-`(
    cbind.data.frame(results_coef_for_table
                     , y$x1, y$s1, y$x2, y$s2
                     , y$d_paired, y$ci_upper, y$ci_lower
                     , y$vd, icc$ICC_unadjusted)
    ,names_for_table)

rm(y)

s3bad_rtot_vars <- as.data.frame(VarCorr(model1))

```


We conducted a linear-mixed-effects model to test if condition influenced MPS-4 responses. Our outcome measure was MPS-4, our predictor variable was condition; we allowed intercepts and the effect of condition to vary across participants. Overall, the model significantly predicted participants responses, and provided a better fit for the data than the baseline model, $\chi$^2^(`r results_anova$Df[2]`) = `r results_anova$Chisq[2]`, *p* `r paste(p_report(p1))`. Condition significantly influenced MPS-4 responses *F*(`r aov1$NumDF[1]`, `r aov1$DenDF[1]`) = `r f3`, *p* `r paste(p_report(p3))`, and was a significant predictor in the model $b$ = `r results_coef$Estimate[2]` ($b$~standardized~ = `r b_std`), *t*(`r results_coef$df[2]`) = `r t1`, *p* `r paste(p_report(p2))`, (*d* = `r round(d, digits=2)`), see Figure\ \@ref(fig:S3bothconditionplot).





```{r}
x <- df3
df4 <- df_long_extra_clean
x <- df_long_extra_clean

x <- x[which(x$valence=="bad"),]


model0 <- lmerTest::lmer(R_tot ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
                #, contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
model1 <- lmerTest::lmer(R_tot ~
                  condition*scenario
                + (1|ResponseId)
             #   + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  , scenario = contr.sum)
            )
anova(model1)
summary(model1)




model1_std <- lmerTest::lmer(scale(M1, scale = TRUE) ~
                  condition*scenario
                + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  , scenario = contr.sum)
            )
summary(model1_std)
b_std <- as.data.frame(summary(model1_std)$coefficients)$Estimate[2]

results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)


results_coef <- as.data.frame(summary_model1$coefficients)

aov1 <- anova(model1)
f3 <- aov1$`F value`[1]
p3 <- aov1$`Pr(>F)`[1]


results_coef$Estimate[2]
results_coef$`Std. Error`[2]
results_coef$df[2]
results_coef$`t value`[2]
t1 <- results_coef$`t value`[2]

p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)



```


```{r}
x <- df3

x <- x[which(x$valence=="bad"),]


model0 <- lmerTest::lmer(M1 ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
                #, contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
model1 <- lmerTest::lmer(M1 ~
                  condition*scenario
                + (1|ResponseId)
             #   + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  , scenario = contr.sum)
            )
anova(model1)
summary(model1)
results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)


results_coef <- as.data.frame(summary_model1$coefficients)

aov1 <- anova(model1)
f3 <- aov1$`F value`[1]
p3 <- aov1$`Pr(>F)`[1]


results_coef$Estimate[2]
results_coef$`Std. Error`[2]
results_coef$df[2]
results_coef$`t value`[2]
t1 <- results_coef$`t value`[2]

p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)

# d <- (mean(x$M1[which(x$condition=="diagnostic")])-mean(x$M1[which(x$condition=="non-diagnostic")]))/sd(x$M1)


s_pooled_fun <- function(){
  
  x$dv <- x$M1
  
  
  z <- 1.959964
  
  df_1 <- x[which(x$condition=="diagnostic"),]
  df_2 <- x[which(x$condition=="non-diagnostic"),]
  
  
  df_1 <- df_1[order(df_1$ResponseId), ]
  df_2 <- df_2[order(df_2$ResponseId), ]
  
  x1 <- mean(df_1$dv)
  x2 <- mean(df_2$dv)
  
  n1 <- length(df_1$gender)
  n2 <- length(df_2$gender)
  
  s1 <- sd(df_1$dv)
  s2 <- sd(df_2$dv)
  
  s_pooled <- sqrt(
    (
      ((s1*s1)*(n1-1))+((s2*s2)*(n2-1))
    )/
      (n1+n1-2)
  )
  
  d <- (x1-x2)/s_pooled
  
  
  d_paired <- (
    mean(df_1$dv-df_2$dv)/
      sd(df_1$dv-df_2$dv)
  )
  
  d <- d_paired
  
  vd <- ((n1+n2)/(n1*n2))+((d*d)/(2*(n1+n2)))
  
  sed <- sqrt(vd)
  
  ci_upper <- d+(z*sed)
  ci_lower <- d-(z*sed)
  
  d_paired <- (
    mean(df_1$dv-df_2$dv)/
      sd(df_1$dv-df_2$dv)
  )
  
  
  n <- length(x$gender)/4
  var <- (1/n)+((d_paired*d_paired)/(2*n))
  vd <- var
  sed <- sqrt(vd)
  
  
  ci_upper <- d_paired+(z*sed)
  ci_lower <- d_paired-(z*sed)
  
  list(study="Study 3",
       x1=x1
       ,x2=x2
       ,s1=s1
       ,s2=s2
       ,n1=n1
       ,n2=n2
       ,s_pooled=s_pooled
       ,d=d
       ,vd=vd
       ,sed=sed
       ,ci_upper=ci_upper
       ,ci_lower=ci_lower
       ,p=p2
       ,d_paired=d_paired)
     
}

s3bad_M1_stats <- s_pooled_fun()
d <- s3bad_M1_stats$d
d


```


```{r}

y <- s3bad_M1_stats

icc <- performance::icc(model1)
icc$ICC_unadjusted

names_for_table <- c("b", "SE", "df", "t", "p"
  , "Mdiag", "SDdiag", "Mnond", "SDnond"
  , "d", "upper","lower","variance","ICC")

results_coef_for_table <- results_coef[2,]
class(results_coef_for_table)

s3bad_M1_for_table <- 
  `colnames<-`(
    cbind.data.frame(results_coef_for_table
                     , y$x1, y$s1, y$x2, y$s2
                     , y$d_paired, y$ci_upper, y$ci_lower
                     , y$vd, icc$ICC_unadjusted)
    ,names_for_table)

rm(y)


s3bad_M1_vars <- as.data.frame(VarCorr(model1))

save(s3bad_rtot_for_table, s3bad_M1_for_table, s3bad_rtot_vars, s3bad_M1_vars, file = "s3bad_table_data.RData")


```


We conducted a linear-mixed-effects model to test if condition influenced MM-1 responses. Our outcome measure was MM-1, our predictor variable was condition; we allowed intercepts and the effect of condition to vary across participants. Overall, the model significantly predicted participants responses, and provided a better fit for the data than the baseline model, $\chi$^2^(`r results_anova$Df[2]`) = `r results_anova$Chisq[2]`, *p* `r paste(p_report(p1))`. Condition significantly influenced MM-1 responses *F*(`r aov1$NumDF[1]`, `r aov1$DenDF[1]`) = `r f3`, *p* `r paste(p_report(p3))`, and was a significant predictor in the model $b$ = `r results_coef$Estimate[2]` ($b$~standardized~ = `r b_std`), *t*(`r results_coef$df[2]`) = `r t1`, *p* `r paste(p_report(p2))`, (*d* = `r round(d, digits=2)`), see Figure\ \@ref(fig:S3bothconditionplot).



## Differences in the *Good* Descriptions



```{r}
x <- df3

x <- x[which(x$valence=="good"),]


model0 <- lmerTest::lmer(R_tot ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
                #, contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
model1 <- lmerTest::lmer(R_tot ~
                  condition*scenario
                + (1|ResponseId)
             #   + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  , scenario = contr.sum)
            )
anova(model1)
summary(model1)


model1_std <- lmerTest::lmer(scale(R_tot, scale = TRUE) ~
                  condition*scenario
                + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  , scenario = contr.sum)
            )
summary(model1_std)
b_std <- as.data.frame(summary(model1_std)$coefficients)$Estimate[2]



results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)


results_coef <- as.data.frame(summary_model1$coefficients)

aov1 <- anova(model1)
f3 <- aov1$`F value`[1]
p3 <- aov1$`Pr(>F)`[1]


results_coef$Estimate[2]
results_coef$`Std. Error`[2]
results_coef$df[2]
results_coef$`t value`[2]
t1 <- results_coef$`t value`[2]

p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)



# d <- (mean(x$R_tot[which(x$condition=="diagnostic")])-mean(x$R_tot[which(x$condition=="non-diagnostic")]))/sd(x$R_tot)



s_pooled_fun <- function(){
  
  x$dv <- x$R_tot
  
 
  z <- 1.959964
  
  df_1 <- x[which(x$condition=="diagnostic"),]
  df_2 <- x[which(x$condition=="non-diagnostic"),]
  
  
  df_1 <- df_1[order(df_1$ResponseId), ]
  df_2 <- df_2[order(df_2$ResponseId), ]
  
  x1 <- mean(df_1$dv)
  x2 <- mean(df_2$dv)
  
  n1 <- length(df_1$gender)
  n2 <- length(df_2$gender)
  
  s1 <- sd(df_1$dv)
  s2 <- sd(df_2$dv)
  
  s_pooled <- sqrt(
    (
      ((s1*s1)*(n1-1))+((s2*s2)*(n2-1))
    )/
      (n1+n1-2)
  )
  
  d <- (x1-x2)/s_pooled
  
  
  d_paired <- (
    mean(df_1$dv-df_2$dv)/
      sd(df_1$dv-df_2$dv)
  )
  
  d <- d_paired
  
  vd <- ((n1+n2)/(n1*n2))+((d*d)/(2*(n1+n2)))
  
  sed <- sqrt(vd)
  
  ci_upper <- d+(z*sed)
  ci_lower <- d-(z*sed)
  
  d_paired <- (
    mean(df_1$dv-df_2$dv)/
      sd(df_1$dv-df_2$dv)
  )
  
  
  n <- length(x$gender)/4
  var <- (1/n)+((d_paired*d_paired)/(2*n))
  vd <- var
  sed <- sqrt(vd)
  
  
  ci_upper <- d_paired+(z*sed)
  ci_lower <- d_paired-(z*sed)
  
  list(study="Study 3",
       x1=x1
       ,x2=x2
       ,s1=s1
       ,s2=s2
       ,n1=n1
       ,n2=n2
       ,s_pooled=s_pooled
       ,d=d
       ,vd=vd
       ,sed=sed
       ,ci_upper=ci_upper
       ,ci_lower=ci_lower
       ,p=p2
       ,d_paired=d_paired)
     
}

s3good_rtot_stats <- s_pooled_fun()
d <- s3good_rtot_stats$d

```



```{r}

y <- s3good_rtot_stats

icc <- performance::icc(model1)
icc$ICC_unadjusted

names_for_table <- c("b", "SE", "df", "t", "p"
  , "Mdiag", "SDdiag", "Mnond", "SDnond"
  , "d", "upper","lower","variance","ICC")

results_coef_for_table <- results_coef[2,]
class(results_coef_for_table)

s3good_rtot_for_table <- 
  `colnames<-`(
    cbind.data.frame(results_coef_for_table
                     , y$x1, y$s1, y$x2, y$s2
                     , y$d_paired, y$ci_upper, y$ci_lower
                     , y$vd, icc$ICC_unadjusted)
    ,names_for_table)

rm(y)


s3good_rtot_vars <- as.data.frame(VarCorr(model1))

```

We conducted a linear-mixed-effects model to test if condition influenced MPS-4 responses. Our outcome measure was MPS-4, our predictor variable was condition; we allowed intercepts and the effect of condition to vary across participants. Overall, the model significantly predicted participants responses, and provided a better fit for the data than the baseline model, $\chi$^2^(`r results_anova$Df[2]`) = `r results_anova$Chisq[2]`, *p* `r paste(p_report(p1))`. Condition significantly influenced MPS-4 responses *F*(`r aov1$NumDF[1]`, `r aov1$DenDF[1]`) = `r f3`, *p* `r paste(p_report(p3))`, and was a significant predictor in the model $b$ = `r results_coef$Estimate[2]` ($b$~standardized~ = `r b_std`), *t*(`r results_coef$df[2]`) = `r t1`, *p* `r paste(p_report(p2))`, (*d* = `r round(d, digits=2)`), see Figure\ \@ref(fig:S3bothconditionplot).



```{r}
x <- df3

x$Valence <- dplyr::recode(x$valence
              , "good" = "Good"
              , "bad" = "Bad")

```



```{r}

#install.packages("ggplot2")
#library(ggplot2)

table(x$condition,x$Valence)

#x <- na.omit(x) 


x_error_bars <- Rmisc::summarySE(x, measurevar="R_tot", groupvars=c("condition", "Valence"))
x_error_bars

g <- ggplot(x,
            aes(x = condition, y = R_tot
                , fill=factor(condition
                              ,labels=c("Diagnostic","Non-Diagnostic")
                )
               # , color=factor(Valence)
            )) + 
  
  #scale_y_continuous(limits = c(-0, 140))+
  #, labels = percent_format()
  # )+ 
  ggdist::stat_halfeye(
    adjust = 2.1, 
    width = .35, 
    .width = 0, 
    justification = -.3, 
    point_colour = NA,
    position = position_dodge(width=0.9)
  ) + 
  geom_boxplot(
    width = .13, 
    outlier.shape = NA
    , position = position_dodge(width=.8)
    , show_guide = FALSE
  ) +
  ## add justified jitter from the {gghalves} package
  gghalves::geom_half_point(
    aes(color=factor(condition
                     #                   ,labels=c("Means","Side-Effect")
    )
    ),
    position = position_dodge(width=4.3),
    ## draw jitter on the left
    side = "l", 
    ## control range of jitter
    range_scale = .4, 
    ## add some transparency
    alpha = .42,
    size = .1,
    show_guide = FALSE
  )+
  stat_summary(
    aes(color=factor(Condition
                                          #,labels=c("Gratitude","No Gratitude")
    )
    ),
    geom = "point",
    fun = "mean",
    col = "black",
    size = 1,
    shape = 16, #24,
    position = position_dodge(width=.2)
   # , justification = -.2
    , fill = "black"
    ,show_guide = FALSE
  )+
  geom_errorbar(data = x_error_bars,aes(ymin=R_tot-se
                                        , ymax=R_tot+se
                                        , width=.05#, position=pd
                                        # , fill= "black" #factor(condition
                                        #     ,labels=c("Means","Side-Effect")
                                        #)
  )
  , position = position_dodge(width=6.12)
  ) +
  #facet_grid(cols = vars(means))+
  xlab("Information Type") +
  ylab("Moral Perception Scale (MPS-4)")+
  scale_x_discrete(
    labels=c("Diagnostic","Non-Diagnostic")
  ) +
  scale_y_continuous(limits = c(-.0,7)
                     , breaks = seq(0,7, by = 1)  #  c(0:)
                     #                    , labels=c(" ", "1", "2", "3","4","5","6","7")
  ) +
  scale_fill_grey(start = .3, end = .6) +
  scale_color_grey(start = .3, end = .6) +
  guides(
    fill=guide_legend(title = "Information Type")
    , color=guide_legend(title="Valence")
    , shape="none"
  )+
  facet_grid(cols = vars(Valence))+
  #labs(fill="Means/Side-Effect") +
  # 
  # geom_text(#family = "Times",
  #           size=4.2,
  #           aes( label = scales::percent(test$perc),
  #                y= test$perc ),
  #           stat= "identity",
  #           vjust = -.5,
  #           position = position_dodge(.9),
  #           fontface='plain'
  #           )+
#theme_apa() +
  theme_bw() +
  theme(panel.border = element_blank(),
        axis.line = element_line(size = .2),
        strip.background  = element_blank(),
        #panel.grid = element_blank(),
        plot.title=element_text(#family="Times",
                                size=12
                                ),
        legend.text=element_text(#family="Times",
                                 size=8
                                 ),
          legend.title=element_text(#family="Times",
                                    size=10
                                    ),
          axis.text=element_text(#family="Times",
                                 colour = "black",
                                 size=8
                                 ),
          axis.ticks.x = element_blank(),
          axis.title=element_text(#family="Times",
                                  size=12
                                  ),
          strip.text=element_text(#family = "Times",
                                  size = 12
                                  ),
         # strip.background = element_rect(fill = "white"),
          legend.position="none")
g
mps4plot <- g

```


```{r S3Rtotconditionplot, fig.cap="Study 3: Differences in MPS-4 depending on condition", include=FALSE}

# mps4plot <- 
# ggplot(x,aes(x=condition,y=R_tot
#              ,fill=condition #Valence
#              ,color= Valence #condition
#              ))+
#   geom_violin(color='black') +
#   
#   
# #  geom_dotplot(binaxis='y', stackdir='center', dotsize=.05)+
# # violin plot with jittered points
# # 0.2 : degree of jitter in x direction
#   geom_jitter(shape=16
#               , position=position_jitter(0.15)
#               , size=.1
#               #, color="dark grey"
#               #, fill="white"
#               ) +
#   geom_boxplot(width=0.2
#                #,color='black'
#                , fill="white"
#                )+
#   geom_jitter(shape=16
#               , position=position_jitter(0.15)
#               , size=.1
#               #, color="dark grey"
#               #, fill="white"
#               ) +
#   stat_summary(fun=mean, geom="point", shape=23, size=4,color="black")+
#   
#   facet_grid(cols = vars(Valence)) +
#   #scale_fill_manual(values = c("lightpink","lightblue"))+
#   scale_fill_manual(values = c('yellow','purple'))+
#   scale_color_manual(values = c("lightpink","lightblue"))+
#   #scale_color_manual(values = c("darkred","navyblue"))+
#   #scale_color_manual(values = c('purple','darkgrey'))+
#   xlab("Condition") +
#   ylab("MPS-4") +
#   theme_bw() +
#   theme(panel.border = element_blank(),
#         axis.line = element_line(size = .2),
#         strip.background  = element_blank(),
#         #panel.grid = element_blank(),
#         plot.title=element_text(#family="Times",
#                                 size=12
#                                 ),
#         legend.text=element_text(#family="Times",
#                                  size=8
#                                  ),
#           legend.title=element_text(#family="Times",
#                                     size=10
#                                     ),
#           axis.text=element_text(#family="Times",
#                                  colour = "black",
#                                  size=8
#                                  ),
#           axis.ticks.x = element_blank(),
#           axis.title=element_text(#family="Times",
#                                   size=12
#                                   ),
#           strip.text=element_text(#family = "Times",
#                                   size = 12
#                                   ),
#          # strip.background = element_rect(fill = "white"),
#           legend.position="none")
# 
# 
# mps4plot

```


```{r}
#x <- na.omit(x) 

# x$Condition1
# 
# 
# x$Condtion <- as.factor(x$Condition)
# 
# 
# x$Condition <- car::recode(x$Condition, "1 = 'Gratitude'; 2 = 'No Gratitude'")

x_error_bars <- Rmisc::summarySE(x, measurevar="M1", groupvars=c("condition", "Valence"))
x_error_bars

g <- ggplot(x,
            aes(x = condition, y = M1
                , fill=factor(condition
                              ,labels=c("Diagnostic","Non-Diagnostic")
                )
               # , color=factor(Valence)
            )) + 
  
  #scale_y_continuous(limits = c(-0, 140))+
  #, labels = percent_format()
  # )+ 
  ggdist::stat_halfeye(
    adjust = 2.1, 
    width = .35, 
    .width = 0, 
    justification = -.3, 
    point_colour = NA,
    position = position_dodge(width=0.9)
  ) + 
  geom_boxplot(
    width = .13, 
    outlier.shape = NA
    , position = position_dodge(width=.8)
    , show_guide = FALSE
  ) +
  ## add justified jitter from the {gghalves} package
  gghalves::geom_half_point(
    aes(color=factor(condition
                     #                   ,labels=c("Means","Side-Effect")
    )
    ),
    position = position_dodge(width=4.3),
    ## draw jitter on the left
    side = "l", 
    ## control range of jitter
    range_scale = .4, 
    ## add some transparency
    alpha = .42,
    size = .1,
    show_guide = FALSE
  )+
  stat_summary(
    aes(color=factor(Condition
                                          #,labels=c("Gratitude","No Gratitude")
    )
    ),
    geom = "point",
    fun = "mean",
    col = "black",
    size = 1,
    shape = 16, #24,
    position = position_dodge(width=.2)
   # , justification = -.2
    , fill = "black"
    ,show_guide = FALSE
  )+
  geom_errorbar(data = x_error_bars,aes(ymin=M1-se
                                        , ymax=M1+se
                                        , width=.05#, position=pd
                                        # , fill= "black" #factor(condition
                                        #     ,labels=c("Means","Side-Effect")
                                        #)
  )
  , position = position_dodge(width=6.12)
  ) +
  #facet_grid(cols = vars(means))+
  xlab("Information Type") +
  ylab("Moral Perception Measure (MM-1)")+
  scale_x_discrete(
    labels=c("Diagnostic","Non-Diagnostic")
  ) +
  scale_y_continuous(limits = c(-.0,100)
                     , breaks = seq(0,100, by = 10)  #  c(0:)
                     #                    , labels=c(" ", "1", "2", "3","4","5","6","7")
  ) +
  scale_fill_grey(start = .3, end = .6) +
  scale_color_grey(start = .3, end = .6) +
  guides(
    fill=guide_legend(title = "Information Type")
    , color=guide_legend(title="Valence")
    , shape="none"
  )+
  facet_grid(cols = vars(Valence))+
  #labs(fill="Means/Side-Effect") +
  # 
  # geom_text(#family = "Times",
  #           size=4.2,
  #           aes( label = scales::percent(test$perc),
  #                y= test$perc ),
  #           stat= "identity",
  #           vjust = -.5,
  #           position = position_dodge(.9),
  #           fontface='plain'
  #           )+
#theme_apa() +
  theme_bw() +
  theme(panel.border = element_blank(),
        axis.line = element_line(size = .2),
        strip.background  = element_blank(),
        #panel.grid = element_blank(),
        plot.title=element_text(#family="Times",
                                size=12
                                ),
        legend.text=element_text(#family="Times",
                                 size=8
                                 ),
          legend.title=element_text(#family="Times",
                                    size=10
                                    ),
          axis.text=element_text(#family="Times",
                                 colour = "black",
                                 size=8
                                 ),
          axis.ticks.x = element_blank(),
          axis.title=element_text(#family="Times",
                                  size=12
                                  ),
          strip.text=element_text(#family = "Times",
                                  size = 12
                                  ),
         # strip.background = element_rect(fill = "white"),
          legend.position="none")

g

M1plot <- g
```


```{r S3M1conditionplot, fig.cap="Study 3: Differences in MM1 depending on condition", include=FALSE, out.width = "50%"}
# 
# M1plot <- 
#   ggplot(x,aes(x=condition,y=M1
#              ,fill=condition #Valence
#              ,color= Valence#condition
#              ))+
#     geom_violin(color='black') +
#   
#   
# #  geom_dotplot(binaxis='y', stackdir='center', dotsize=.05)+
# # violin plot with jittered points
# # 0.2 : degree of jitter in x direction
#   geom_jitter(shape=16
#               , position=position_jitter(0.15)
#               , size=.1
#               #, color="dark grey"
#               #, fill="white"
#               ) +
#   geom_boxplot(width=0.2
#                #,color='black'
#                , fill="white"
#                )+
#   geom_jitter(shape=16
#               , position=position_jitter(0.15)
#               , size=.1
#               #, color="dark grey"
#               #, fill="white"
#               ) +
#   stat_summary(fun=mean, geom="point", shape=23, size=4,color="black")+
#   
#   facet_grid(cols = vars(Valence)) +
#   #scale_fill_manual(values = c("lightpink","lightblue"))+
#   scale_fill_manual(values = c('yellow','purple'))+
#   scale_color_manual(values = c("lightpink","lightblue"))+
#   #scale_color_manual(values = c("darkred","navyblue"))+
#   #scale_color_manual(values = c('purple','darkgrey'))+
#   xlab("Condition") +
#   ylab("MM-1")+
#   facet_grid(cols = vars(Valence)) +
#   theme_bw() +
#   theme(panel.border = element_blank(),
#         axis.line = element_line(size = .2),
#         strip.background  = element_blank(),
#         #panel.grid = element_blank(),
#         plot.title=element_text(#family="Times",
#           size=12
#         ),
#         legend.text=element_text(#family="Times",
#           size=8
#         ),
#         legend.title=element_text(#family="Times",
#           size=10
#         ),
#         axis.text=element_text(#family="Times",
#           colour = "black",
#           size=8
#         ),
#         axis.ticks.x = element_blank(),
#         axis.title=element_text(#family="Times",
#           size=12
#         ),
#         strip.text=element_text(#family = "Times",
#           size = 12
#         ),
#         # strip.background = element_rect(fill = "white"),
#         legend.position="none")
# 



```

```{r}
figure <- ggarrange(mps4plot
                    , M1plot
                    #,labels = c("A")
                    , ncol = 1
                    , nrow = 2
                    #, widths = c(0.485, 0.5)
                    )
figure

ggsave("Study3.png",figure)
```


```{r S3bothconditionplot, fig.cap="Study 3: Differences in moral perception depending on condition", include=TRUE, fig.width = 5, fig.height = 7.5}

suppressWarnings(print(figure))

```



```{r}
x <- df3

x <- x[which(x$valence=="good"),]


model0 <- lmerTest::lmer(M1 ~ 1
                #   condition
                 + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
                #, contrasts = list(condition = contr.sum  , valence = contr.sum)
            )

summary(model0)
model1 <- lmerTest::lmer(M1 ~
                  condition*scenario
                + (1|ResponseId)
             #   + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  , scenario = contr.sum)
            )
anova(model1)
summary(model1)



model1_std <- lmerTest::lmer(scale(M1, scale = TRUE) ~
                  condition*scenario
                + (1|ResponseId)
                # + (1|ResponseId:condition)
                , data = x
                , contrasts = list(condition = contr.sum  , scenario = contr.sum)
            )
summary(model1_std)
b_std <- as.data.frame(summary(model1_std)$coefficients)$Estimate[2]




results_anova <- as.data.frame(anova(model0,model1))
results_anova
results_anova$Chisq[2]
results_anova$Df
p1 <- results_anova$`Pr(>Chisq)`[2]
p_report(p1)
#p_report(results_anova$`Pr(>Chisq)`)[2]
results_anova$AIC
summary_model1 <- summary(model1)


results_coef <- as.data.frame(summary_model1$coefficients)

aov1 <- anova(model1)
f3 <- aov1$`F value`[1]
p3 <- aov1$`Pr(>F)`[1]


results_coef$Estimate[2]
results_coef$`Std. Error`[2]
round(results_coef$df[2])
results_coef$`t value`[2]
t1 <- results_coef$`t value`[2]

p2 <- results_coef$`Pr(>|t|)`[2]

#QuantPsyc::lm.beta(results_coef)
anova(model1)

# d <- (mean(x$M1[which(x$condition=="diagnostic")])-mean(x$M1[which(x$condition=="non-diagnostic")]))/sd(x$M1)


s_pooled_fun <- function(){
  
  x$dv <- x$M1
  
  
  z <- 1.959964
  
  df_1 <- x[which(x$condition=="diagnostic"),]
  df_2 <- x[which(x$condition=="non-diagnostic"),]
  
  
  df_1 <- df_1[order(df_1$ResponseId), ]
  df_2 <- df_2[order(df_2$ResponseId), ]
  
  x1 <- mean(df_1$dv)
  x2 <- mean(df_2$dv)
  
  n1 <- length(df_1$gender)
  n2 <- length(df_2$gender)
  
  s1 <- sd(df_1$dv)
  s2 <- sd(df_2$dv)
  
  s_pooled <- sqrt(
    (
      ((s1*s1)*(n1-1))+((s2*s2)*(n2-1))
    )/
      (n1+n1-2)
  )
  
  d <- (x1-x2)/s_pooled
  
  
  d_paired <- (
    mean(df_1$dv-df_2$dv)/
      sd(df_1$dv-df_2$dv)
  )
  
  d <- d_paired
  
  vd <- ((n1+n2)/(n1*n2))+((d*d)/(2*(n1+n2)))
  
  sed <- sqrt(vd)
  
  ci_upper <- d+(z*sed)
  ci_lower <- d-(z*sed)
  
  d_paired <- (
    mean(df_1$dv-df_2$dv)/
      sd(df_1$dv-df_2$dv)
  )
  
  
  n <- length(x$gender)/4
  var <- (1/n)+((d_paired*d_paired)/(2*n))
  vd <- var
  sed <- sqrt(vd)
  
  
  ci_upper <- d_paired+(z*sed)
  ci_lower <- d_paired-(z*sed)
  
  list(study="Study 3",
       x1=x1
       ,x2=x2
       ,s1=s1
       ,s2=s2
       ,n1=n1
       ,n2=n2
       ,s_pooled=s_pooled
       ,d=d
       ,vd=vd
       ,sed=sed
       ,ci_upper=ci_upper
       ,ci_lower=ci_lower
       ,p=p2
       ,d_paired=d_paired)
     
}

s3good_M1_stats <- s_pooled_fun()
d <- s3good_M1_stats$d
d

save(s3good_rtot_stats
     , s3good_M1_stats
     , s3bad_rtot_stats
     , s3bad_M1_stats, file = "S3_effect_stats.RData")


```


```{r}

y <- s3good_M1_stats

icc <- performance::icc(model1)
icc$ICC_unadjusted

names_for_table <- c("b", "SE", "df", "t", "p"
  , "Mdiag", "SDdiag", "Mnond", "SDnond"
  , "d", "upper","lower","variance","ICC")

results_coef_for_table <- results_coef[2,]
class(results_coef_for_table)

s3good_M1_for_table <- 
  `colnames<-`(
    cbind.data.frame(results_coef_for_table
                     , y$x1, y$s1, y$x2, y$s2
                     , y$d_paired, y$ci_upper, y$ci_lower
                     , y$vd, icc$ICC_unadjusted)
    ,names_for_table)

rm(y)

s3good_M1_vars <- as.data.frame(VarCorr(model1))

save(s3good_rtot_for_table, s3good_M1_for_table, s3good_rtot_vars, s3good_M1_vars, file = "s3good_table_data.RData")


```


We conducted a linear-mixed-effects model to test if condition influenced MM-1 responses. Our outcome measure was MM-1, our predictor variable was condition; we allowed intercepts and the effect of condition to vary across participants. Overall, the model significantly predicted participants responses, and provided a better fit for the data than the baseline model, $\chi$^2^(`r results_anova$Df[2]`) = `r results_anova$Chisq[2]`, *p* `r paste(p_report(p1))`. Condition significantly influenced MM-1 responses *F*(`r aov1$NumDF[1]`, `r round(aov1$DenDF[1])`) = `r f3`, *p* `r paste(p_report(p3))`, and was a significant predictor in the model $b$ = `r results_coef$Estimate[2]` ($b$~standardized~ = `r b_std`), *t*(`r round(results_coef$df[2])`) = `r t1`, *p* `r paste(p_report(p2))`, (*d* = `r round(d, digits=2)`), see Figure\ \@ref(fig:S3bothconditionplot).



